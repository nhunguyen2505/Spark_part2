{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RDD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnDHQU_zgbxA",
        "outputId": "41df872a-d0d6-47e6-dbe0-56092c2314a5"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=1f516bdce3db9b570a18ef78d9760325b57022da3316ad696aaaf5ccaebad7f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUNF65Zmg7Fd"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "import collections"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3jHVxFng9uu",
        "outputId": "a33be82f-e049-4b02-d406-1ae6e8773b8f"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Python Spark create RDD example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\r\n",
        "\r\n",
        "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\r\n",
        "             (4, 5, 6, 'd e f'),\r\n",
        "             (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\r\n",
        "df.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+----+-----+\n",
            "|col1|col2|col3| col4|\n",
            "+----+----+----+-----+\n",
            "|   1|   2|   3|a b c|\n",
            "|   4|   5|   6|d e f|\n",
            "|   7|   8|   9|g h i|\n",
            "+----+----+----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD60kYP0ir_B",
        "outputId": "fddb6df6-84e7-4e58-e2cd-d1668eace053"
      },
      "source": [
        "dt = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\r\n",
        "dt.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35olwzaPlD81",
        "outputId": "311111d7-6253-450a-d4ab-722fbeb548e5"
      },
      "source": [
        "total_dt = dt.reduce(lambda x,y: x+y)\r\n",
        "total_dt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlDrCaRRl9QW",
        "outputId": "1da07c1c-6056-46be-bd1b-8d6ed27cb2fb"
      },
      "source": [
        "conf = SparkConf().setMaster('local').setAppName('Word counting')\r\n",
        "sc = SparkContext.getOrCreate(conf = conf)\r\n",
        "text = \"to be or not to be\".split()\r\n",
        "rdd = sc.parallelize(text)\r\n",
        "counts = rdd.map(lambda x:(x,1))\r\n",
        "print(counts.collect())\r\n",
        "\r\n",
        "red = counts.reduceByKey(lambda x,y: x+y)\r\n",
        "print(red.collect())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]\n",
            "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E7dG3gkmGiD",
        "outputId": "f6afc0cc-ed34-4016-bf65-f4389f2d8cb9"
      },
      "source": [
        "conf = SparkConf().setMaster('local').setAppName('Arr')\r\n",
        "sc = SparkContext.getOrCreate(conf = conf)\r\n",
        "Arr = [3,5,6,7]\r\n",
        "rdd = sc.parallelize(Arr)\r\n",
        "max_i = rdd.reduce(lambda x,y: max(x,y))\r\n",
        "print(max_i)\r\n",
        "#counts = rdd.map(lambda x:(x,1))\r\n",
        "aver = rdd.reduce(lambda a, b: a + b) / len(Arr)\r\n",
        "print(aver)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "5.25\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}